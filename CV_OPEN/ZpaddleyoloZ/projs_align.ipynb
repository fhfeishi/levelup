{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **就是说记录一下如何使用PaddleYOLO这个仓库**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. 数据集对齐PaddleYOLO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# my voc-OD-dataset structure\n",
    "VOC_2007:\n",
    "~/VOCDevkit/VOC2007\n",
    "    - /Annotations   # *.xml\n",
    "    - /ImageSets\n",
    "        - /Main      # *.txt\n",
    "    - /JPEGImages    # *.jpg\n",
    "\n",
    "# target voc-OD-dataset structure\n",
    "paddledet_voc:\n",
    "~/dataset/paddledet_voc\n",
    "    - label_list.txt\n",
    "    - trainval.txt   :path/to/(*.jpg *.xml) VOCDevkit/VOC2007/*.jpg VOCDevkit/VOC2007/\n",
    "    - test.txt       :path/to/(*.jpg *.xml) VOCDevkit/VOC2007/*.jpg VOCDevkit/VOC2007/\n",
    "    - train.txt      :path/to/(*.jpg *.xml) VOCDevkit/VOC2007/*.jpg VOCDevkit/VOC2007/\n",
    "    - val.txt        :path/to/(*.jpg *.xml) VOCDevkit/VOC2007/*.jpg VOCDevkit/VOC2007/\n",
    "    - /VOCDevkit/2007\n",
    "        - /Annotations  # *.xml   \n",
    "        - /ImageSets\n",
    "            - /Main     # *.txt  # line:path/to/data/*  \n",
    "        - /JPEGImages   # *.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "paddledet_vocdataset_root = r\"dataset/paddledet_voc\"\n",
    "os.makedirs(paddledet_vocdataset_root, exist_ok=True)\n",
    "\n",
    "def copy_dirs(src_dir, dst_dir, move=False):\n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        # 计算相对路径\n",
    "        relative_path = os.path.relpath(root, src_dir)\n",
    "        # 创建目标文件夹中的子文件夹\n",
    "        dst_subdir = os.path.join(dst_dir, relative_path)\n",
    "        if not os.path.exists(dst_subdir):\n",
    "            os.makedirs(dst_subdir)\n",
    "        \n",
    "        # 复制文件\n",
    "        for file in files:\n",
    "            src_file = os.path.join(root, file)\n",
    "            dst_file = os.path.join(dst_subdir, file)\n",
    "            if move:\n",
    "                shutil.move(src_file, dst_file)\n",
    "            else:\n",
    "                shutil.copyfile(src_file, dst_file)\n",
    "\n",
    "\n",
    "target_dir  =r\"PaddleYOLO/dataset/paddledet_voc\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "copy_dirs(paddledet_vocdataset_root, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename  my_classes.txt to label_list.txt\n",
    "os.rename(os.path.join(target_dir, \"my_classes.txt\"), os.path.join(target_dir, \"label_list.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 用于训练的数据集root: target_dir  =r\"PaddleYOLO/dataset/paddledet_voc\"\n",
    "# JPEGImages: 图片文件夹  os.path.join(target_dir, \"VOCDevkit/VOC2007/JPEGImages\")\n",
    "# Annotations: 标注文件夹 os.path.join(target_dir, \"VOCDevkit/VOC2007/Annotations\")\n",
    "\n",
    "def get_paddetVocTxt(dataset_root=\"PaddleYOLO/dataset/paddledet_voc\", txt_name=\"trainval.txt\"):\n",
    "    txtpth = os.path.join(os.path.join(dataset_root, \"VOCDevkit/VOC2007/ImageSets/Main\"), txt_name)\n",
    "    with open(txtpth, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()    \n",
    "    lists_path = os.path.join(dataset_root, txt_name)\n",
    "    lists = open(lists_path, 'w', encoding='utf-8')\n",
    "    \n",
    "    \n",
    "    for line in lines:\n",
    "        image_line = os.path.normpath(os.path.join(\"VOCDevkit/VOC2007/JPEGImages\", line.strip()))\n",
    "        anno_line = os.path.normpath(os.path.join(\"VOCDevkit/VOC2007/Annotations\", line.strip()))\n",
    "        lists_line = f\"{image_line}.jpg  {anno_line}.xml\\n\"\n",
    "        lists.write(lists_line)\n",
    "    lists.close()\n",
    "\n",
    "get_paddetVocTxt(txt_name=\"trainval.txt\")\n",
    "get_paddetVocTxt(txt_name=\"train.txt\")\n",
    "get_paddetVocTxt(txt_name=\"val.txt\")\n",
    "get_paddetVocTxt(txt_name=\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**就是简单记录一下(现在可能用不上)**<br>\n",
    "从图片文件集images、标注文件集annotaions  来构建我们的数据集train_set val_set test_set\n",
    "\n",
    "```python\n",
    "~/dataset/\n",
    "    - annotaions      # 存放标注文件,如 *.xml\n",
    "    - images          # 存放图像文件,如 *.jpg\n",
    "    - trainval_test_split/\n",
    "        - split_log   # trainval.txt  train.txt val.txt test.txt\n",
    "    - train_set/\n",
    "        - annotaions  # 存放标注文件,如 *.xml\n",
    "        - images      # 存放图像文件,如 *.jpg\n",
    "    - val_set/\n",
    "        - annotaions  # 存放标注文件,如 *.xml\n",
    "        - images      # 存放图像文件,如 *.jpg\n",
    "    - test_set/\n",
    "        - annotaions  # 存放标注文件,如 *.xml\n",
    "        - images      # 存放图像文件,如 *.jpg\n",
    "    - trainval_trainlines.txt  # path/to/data/* annotations_1 annotations_2 ...\n",
    "    - trainval_vallines.txt\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm \n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from utils.utils import get_classes\n",
    "\n",
    "classes_path = r\"model_data/my_classes.txt\"\n",
    "trainval_percent = 0.9\n",
    "train_percent = 0.9\n",
    "dataset_root = \"dataset\"\n",
    "dataset_structure_original = {'image': 'images', 'label': 'annotations'}\n",
    "\n",
    "# -> split_log : trainval.txt train.txt val.txt  test.txt\n",
    "split_log_dir = r\"dataset/split_log\"\n",
    "os.makedirs(split_log_dir, exist_ok=True)\n",
    "\n",
    "dataset_structure_target = [{'sets': {\n",
    "                              'sets': ['train', 'val', 'test'],\n",
    "                              'train': 'dataset/train_set',\n",
    "                              'val': 'dataset/val_set',\n",
    "                              'test': 'dataset/test_set'}},\n",
    "                            {'sets_stru': {'image': 'images', 'label': 'annotations'}}]\n",
    "\n",
    "classes_list, classes_num = get_classes(classes_path)\n",
    "print(len(dataset_structure_target))\n",
    "\n",
    "\n",
    "# split data\n",
    "print(\"Generate txt in splir_log_dir ...\")\n",
    "\n",
    "xmlfilepath = os.path.join(dataset_root, dataset_structure_original['label'])\n",
    "xmlfiles = os.listdir(xmlfilepath)\n",
    "total_xml = []\n",
    "\n",
    "for xml in xmlfiles:\n",
    "    if xml.endswith('.xml'):\n",
    "        total_xml.append(xml)\n",
    "\n",
    "nums = len(total_xml)\n",
    "lists = range(nums)\n",
    "trainval_nums = int(nums * trainval_percent)\n",
    "train_nums = int(trainval_nums * train_percent)\n",
    "trainval = random.sample(lists, trainval_nums)\n",
    "train = random.sample(trainval, train_nums)\n",
    "print(\"train and val size\", trainval_nums)\n",
    "print(\"train size\", train_nums)\n",
    "\n",
    "ftrainval = open(os.path.join(split_log_dir, 'trainval.txt'), 'w', encoding='utf-8')\n",
    "ftest = open(os.path.join(split_log_dir, 'test.txt'), 'w', encoding='utf-8')\n",
    "ftrain = open(os.path.join(split_log_dir, 'train.txt'), 'w', encoding='utf-8')\n",
    "fval = open(os.path.join(split_log_dir, 'val.txt'), 'w', encoding='utf-8')\n",
    "\n",
    "for i in lists:\n",
    "    name = total_xml[i][:-4]+'\\n'  # 仅保留文件名\n",
    "    if i in trainval:\n",
    "        ftrainval.write(name)\n",
    "        if i in train:\n",
    "            ftrain.write(name)\n",
    "        else:\n",
    "            fval.write(name)\n",
    "    else:\n",
    "        ftest.write(name)\n",
    "\n",
    "ftrainval.close()\n",
    "ftrain.close()\n",
    "fval.close()\n",
    "ftest.close()\n",
    "print(\"Generate txt in splir_log_dir done.\")\n",
    "\n",
    "\n",
    "# 移动数据集\n",
    "def get_target_dataset(dataroot = dataset_root,\n",
    "                       log_dir = split_log_dir,\n",
    "                       train_slice='train.txt', \n",
    "                       val_slice='val.txt', \n",
    "                       tes_slice='test.txt', \n",
    "                       target_dataset_stru=dataset_structure_target):\n",
    "    \n",
    "    with open(os.path.join(log_dir, train_slice), 'r', encoding='utf-8') as f:\n",
    "        trains = f.readlines()\n",
    "    with open(os.path.join(log_dir, val_slice), 'r', encoding='utf-8') as f:\n",
    "        vals = f.readlines()\n",
    "    with open(os.path.join(log_dir, tes_slice), 'r', encoding='utf-8') as f:\n",
    "        tests = f.readlines()\n",
    "    \n",
    "    trainset_root = target_dataset_stru[0]['sets']['train']\n",
    "    valset_root = target_dataset_stru[0]['sets']['val']\n",
    "    testset_root = target_dataset_stru[0]['sets']['test']\n",
    "    os.makedirs(trainset_root, exist_ok=True)\n",
    "    os.makedirs(valset_root, exist_ok=True)\n",
    "    os.makedirs(testset_root, exist_ok=True)\n",
    "    \n",
    "    def move_datas(lines, data_root, save_root, stru=dataset_structure_target):\n",
    "        for li in tqdm(lines):\n",
    "            li = li.strip()\n",
    "            # 移动图像文件\n",
    "            image_dir = os.path.join(data_root, stru[1]['sets_stru']['image'])\n",
    "            img_path = os.path.join(image_dir, f\"{li}.jpg\")\n",
    "\n",
    "            if os.path.exists(img_path):\n",
    "                image_save_dir = os.path.join(save_root, stru[1]['sets_stru']['image'])\n",
    "                os.makedirs(image_save_dir, exist_ok=True)\n",
    "                image_save_path = os.path.join(image_save_dir, f\"{li}.jpg\")\n",
    "                shutil.copyfile(img_path, image_save_path)\n",
    "            else:\n",
    "                print(f'{li}.jpg图片文件 is not file !')\n",
    "            \n",
    "            # 移动标签文件\n",
    "            label_dir = os.path.join(data_root, stru[1]['sets_stru']['label'])\n",
    "            xml_path = os.path.join(label_dir, f\"{li}.xml\")\n",
    "            \n",
    "            if os.path.exists(xml_path):\n",
    "                xml_save_dir = os.path.join(save_root, stru[1]['sets_stru']['label'])\n",
    "                os.makedirs(xml_save_dir, exist_ok=True)\n",
    "                xml_save_path = os.path.join(xml_save_dir, f\"{li}.xml\")\n",
    "                shutil.copyfile(xml_path, xml_save_path)\n",
    "            else:\n",
    "                print(f'{li}.xml标注文件 is not file !')\n",
    "    \n",
    "    move_datas(trains, dataroot, trainset_root)\n",
    "    move_datas(vals, dataroot, valset_root)\n",
    "    move_datas(tests,dataroot, testset_root)\n",
    "    \n",
    "    \n",
    "# 生成训练用的： trainval_trainlines.txt   trainval_vallines.txt\n",
    "\n",
    "with open(os.path.join(split_log_dir, 'train.txt'), 'r', encoding='utf-8') as f:\n",
    "    trains = f.readlines()\n",
    "with open(os.path.join(split_log_dir, 'val.txt'), 'r', encoding='utf-8') as f:\n",
    "    vals = f.readlines()\n",
    "\n",
    "def convert_annotations(image_id, linesfile):\n",
    "    # 原始数据的文件夹  xml_dir\n",
    "    infiles = open(os.path.join(\"dataset/paddledet_voc/VOCDevkit/VOC2007/Annotations\", f\"{image_id}.xml\"), 'r', encoding='utf-8')\n",
    "    tree = ET.parse(infiles)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for obj in root.iter('object'):\n",
    "        difficult = 0\n",
    "        if obj.find('difficult') != None:\n",
    "            difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        if cls not in classes_list or int(difficult)==1:\n",
    "            continue\n",
    "        cls_id = classes_list.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (int(float(xmlbox.find('xmin').text)), \n",
    "         int(float(xmlbox.find('ymin').text)), \n",
    "         int(float(xmlbox.find('xmax').text)), \n",
    "         int(float(xmlbox.find('ymax').text)))\n",
    "        # x1 y1 x2 y2 cls_id\n",
    "        linesfile.write(\" \" + \",\".join([str(a) for a in b]) + ',' + str(cls_id))\n",
    "        \n",
    "        # nums[classes_list.index(cls)] = nums[classes_list.index(cls)] + 1\n",
    "\n",
    "\n",
    "def get_lines(dataroot=\"dataset\", splitLogtext=\"dataset/split_log/train.txt\", trainvalLinesText=\"trainval_trainlines.txt\"):\n",
    "    # train.txt or val.txt\n",
    "    \n",
    "    image_ids = open(splitLogtext, 'r', encoding='utf-8').read().strip().split()\n",
    "    listfiles = open(trainvalLinesText, 'w', encoding='utf-8')\n",
    "    \n",
    "    image_dir = os.path.abspath(os.path.join(dataroot, 'images'))\n",
    "    for image_id in image_ids:\n",
    "        image_path = os.path.join(image_dir, f\"{image_id}.jpg\")\n",
    "        listfiles.write(image_path)\n",
    "        convert_annotations(image_id, listfiles)\n",
    "        listfiles.write('\\n')\n",
    "    listfiles.close()\n",
    "    \n",
    "    print(f\"Generate {trainvalLinesText} for train done.\")\n",
    "    \n",
    "    \n",
    "# trainval_trainlines.txt  \n",
    "get_lines(dataroot=\"dataset\", splitLogtext=\"dataset/split_log/train.txt\",trainvalLinesText=\"trainval_trainlines.txt\")\n",
    "\n",
    "\n",
    "# trainval_vallines.txt\n",
    "get_lines(dataroot=\"dataset\", splitLogtext=\"dataset/split_log/val.txt\",trainvalLinesText=\"trainval_vallines.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PaddleYOLO 仓库结构分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# PaddleYOLO\n",
    "~/PaddleYOLO\n",
    "    - /configs    # 各yolo的配置入口 \n",
    "    - /dataset    # 数据集入口，有提供默认数据集的下载、处理脚本\n",
    "    - /demo       # 几张用于测试的图片\n",
    "    - /deploy     # 部署入口\n",
    "    - /docs       # 工程文档入口，教程都在这里\n",
    "    - /ppdet      # import ppdet  # ppdet package源码 (setup.py中有package的构建信息)\n",
    "    - /scripts    # \n",
    "    - /test_tipc  # paddle包装的训练测试开发工具入口\n",
    "    - /tools      # 集成的脚本  如train.py   infer.py  ...\n",
    "    - requirements.txt 、setup.py  # 用于配置/构建PaddleYOLO工程环境\n",
    "    - README.md   # 工程README.md，优先看\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 配置对齐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PaddleYOLO这样的仓库是有学习成本的，但使用逻辑相似。\n",
    "\n",
    "**！！！记住先看工程README.MD ！！！**\n",
    "\n",
    "- 用起来只需要：\n",
    "    - 对齐数据集    --> 仓库的要求格式 \n",
    "    - 构建配置文件  --> 可能需要构建多个.yaml  .yml文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 数据集配置  模型配置 优化器配置 训练配置 验证配置 测试配置  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PaddleYOLO/dataset/configs/datasets/paddledet_voc.yml or .yaml\n",
    "\n",
    "**需要注意一下， yaml文件中就把所有的中文字符删除**\n",
    "\n",
    "```yaml\n",
    "metric: VOC # 目前支持COCO, VOC, WiderFace等评估标准\n",
    "map_type: 11point\n",
    "num_classes: 3 # 数据集的类别数，不包含背景类，roadsign数据集为4类，其他数据需要修改为自己的数据类别\n",
    "\n",
    "TrainDataset:\n",
    "  !VOCDataSet\n",
    "    dataset_dir: dataset/paddledet_voc # 训练集的图片所在文件相对于dataset_dir的路径\n",
    "    anno_path: train.txt # 训练集的标注文件相对于dataset_dir的路径\n",
    "    label_list: label_list.txt # 数据集所在路径，相对于PaddleDetection路径\n",
    "    data_fields: ['image', 'gt_bbox', 'gt_class', 'difficult'] # 控制dataset输出的sample所包含的字段，注意此为训练集Reader独有的且必须配置的字段\n",
    "\n",
    "EvalDataset:\n",
    "  !VOCDataSet\n",
    "    dataset_dir: dataset/paddledet_voc # 数据集所在路径，相对于PaddleDetection路径\n",
    "    anno_path: val.txt # 验证集的标注文件相对于dataset_dir的路径\n",
    "    label_list: label_list.txt # 标签文件，相对于dataset_dir的路径\n",
    "    data_fields: ['image', 'gt_bbox', 'gt_class', 'difficult']\n",
    "\n",
    "TestDataset:\n",
    "  !ImageFolder\n",
    "    anno_path: label_list.txt # 标注文件所在路径，仅用于读取数据集的类别信息，支持json和txt格式\n",
    "    dataset_dir: dataset/paddledet_voc \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新建与训练权重的文件夹,将preweights文件夹移过来<br>\n",
    "PaddleYOLO/configs/yolov8/preweights  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置工程主配置文件\n",
    "```yaml\n",
    "\n",
    "_BASE_: [\n",
    "  '../datasets/p.yml', # 指定为自定义数据集配置路径\n",
    "  '../runtime.yml',\n",
    "  '_base_/optimizer_40e.yml',\n",
    "  '_base_/yolov3_mobilenet_v1.yml',\n",
    "  '_base_/yolov3_reader.yml',\n",
    "]\n",
    "pretrain_weights: configs/yolov8/preweights/yolov8_s_500e_coco.pdparams\n",
    "weights: output/yolov8_s_pdetVOC/model_final\n",
    "\n",
    "YOLOv3Loss:\n",
    "  ignore_thresh: 0.7\n",
    "  label_smooth: true\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```terminal\n",
    "set CUDA_VISIBLE_DEVICES=0 \n",
    "\n",
    "python tools/train.py -c configs/yolov8/yolov8_s_pdetvoc.yml --eval\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppyolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
